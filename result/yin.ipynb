{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from communities.algorithms import louvain_method\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(II,DI):\n",
    "    relation_sums = II.groupby('node_from')['distance'].sum().add(\n",
    "        II.groupby('node_to')['distance'].sum(), fill_value=0\n",
    "    )\n",
    "    relation_sums = pd.DataFrame(relation_sums).reset_index()\n",
    "    relation_sums.columns = ['Compound', 'TotalValue']\n",
    "    total_values_dict = dict(zip(relation_sums['Compound'], relation_sums['TotalValue']))\n",
    "    # 使用apply来计算每一行的total_value\n",
    "    II['total_value1'] = II['node_from'].apply(lambda x: total_values_dict.get(x, 0))\n",
    "    II['total_value2'] = II['node_to'].apply(lambda x: total_values_dict.get(x, 0))\n",
    "    # 计算平均关系值\n",
    "    II['AverageRelationValue'] = II.apply(\n",
    "        lambda row: row['distance'] / (row['total_value1'] * row['total_value2']) if row['distance'] != 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    DI['total_value1'] = DI['node_to'].apply(lambda x: total_values_dict.get(x, 0))\n",
    "    # 计算平均关系值\n",
    "    DI['AverageRelationValue'] = DI.apply(\n",
    "        lambda row: row['distance'] / row['total_value1'] if row['distance'] != 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    II['distance'] = (II['AverageRelationValue'] - II['AverageRelationValue'].min()) / (\n",
    "                II['AverageRelationValue'].max() - II['AverageRelationValue'].min())\n",
    "    DI['distance'] = (DI['AverageRelationValue'] - DI['AverageRelationValue'].min()) / (\n",
    "            DI['AverageRelationValue'].max() - DI['AverageRelationValue'].min())\n",
    "\n",
    "    II = II.iloc[:, [0, 1, 2]]\n",
    "    DI = DI.iloc[:, [0, 1, 2]]\n",
    "    return II,DI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_pre_processing(HIf, HHf, IIf, DHf, DIf):\n",
    "    HI = pd.read_csv(HIf).drop_duplicates()\n",
    "    HH = pd.read_csv(HHf).drop_duplicates()\n",
    "    II = pd.read_csv(IIf).drop_duplicates()\n",
    "    DH = pd.read_csv(DHf).drop_duplicates()\n",
    "    DI = pd.read_csv(DIf).drop_duplicates()\n",
    "    HI = HI.rename(columns={'herb': 'node_from', 'ingredient': 'node_to'})\n",
    "    HI['distance'] = 0\n",
    "    HH = HH.rename(columns={'herb1': 'node_from', 'herb2': 'node_to'})\n",
    "    II['Combination'] = II[['ingre1', 'ingre2']].apply(lambda x: ''.join(sorted(x)), axis=1)\n",
    "    II = II.drop_duplicates(subset=['Combination', 'distance']).drop(columns=['Combination'])\n",
    "    II.columns = ['node_from', 'node_to', 'distance']\n",
    "    DH = DH.iloc[:, [0, 2, 3]].rename(\n",
    "        columns={'Disease': 'node_from', 'Herb name': 'node_to', 'distance': 'distance'}).drop_duplicates()\n",
    "    DI = DI.iloc[:, [0, 4, 5]].rename(\n",
    "        columns={'Disease name': 'node_from', 'Ingredient name': 'node_to', 'distance': 'distance'}).drop_duplicates()\n",
    "    II,DI= scale_data(II,DI)\n",
    "    HH,DH = scale_data(HH,DH)\n",
    "    sum_file = pd.concat([HI, II, DH, DI,HH], axis=0)\n",
    "    return sum_file,DH,DI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_communities(data):\n",
    "    data['distance1'] = 1 - data['distance']\n",
    "    data = data.iloc[:, [0, 1, 3]]\n",
    "    data.columns = ['node1', 'node2', 'weight']\n",
    "    unique_values = pd.unique(data[['node1', 'node2']].values.ravel())\n",
    "    data1 = pd.DataFrame({'node1': unique_values, 'node2': unique_values, 'weight': 1})\n",
    "    data = pd.concat([data1, data], axis=0).drop_duplicates()\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(data)):\n",
    "        node1 = data.iloc[i]['node1']\n",
    "        node2 = data.iloc[i]['node2']\n",
    "        weight = data.iloc[i]['weight']\n",
    "        if not G.has_edge(node1, node2):\n",
    "            G.add_edge(node1, node2, weight=weight)\n",
    "    adj_matrix = nx.to_scipy_sparse_array(G)\n",
    "    adj_matrix_np = adj_matrix.toarray()\n",
    "    communities, frames = louvain_method(adj_matrix_np)\n",
    "    return  G,  adj_matrix_np, communities, frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# communities_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def communities_result(G,communities,DI,DH):\n",
    "    my_list = list(G.nodes())\n",
    "    df = pd.DataFrame(my_list, columns=['Value'])\n",
    "    node_index = my_list.index(\"Cough Variant Asthma\")\n",
    "    print(\"Node 'Cough Variant Asthma' is at index:\", node_index)\n",
    "    for i, s in enumerate(communities):\n",
    "        if node_index in s:\n",
    "            print(\"元素 {} 在第 {} 个集合中.\".format(node_index, i + 1))\n",
    "            break\n",
    "    else:\n",
    "        print(\"元素 {} 不在任何集合中.\".format(node_index))\n",
    "    set = communities[i]\n",
    "    result = df.loc[df.index.isin(set)]\n",
    "    subset_data1= DI[DI['node_to'].isin(result['Value'])]\n",
    "    subset_data2= DH[DH['node_to'].isin(result['Value'])]\n",
    "    return  result,subset_data1,subset_data2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deal_admet_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_admet_file(DIC,ADMET):\n",
    "    ADMET1 = ADMET[['mol_inchikey_dict', 'tcmsp_ingredient_name', 'tcmsp_ingredient_ob', 'tcmsp_ingredient_bbb',\n",
    "                    'tcmsp_ingredient_drug_likeness']]\n",
    "\n",
    "    DIC_ADMET = pd.merge(DIC, ADMET1, left_on='node_to', right_on='tcmsp_ingredient_name',\n",
    "                         how='left').drop_duplicates()\n",
    "    DIC_ADMET = DIC_ADMET[(DIC_ADMET['tcmsp_ingredient_ob'] > 30) & (DIC_ADMET['tcmsp_ingredient_drug_likeness'] > 0.18)]\n",
    "\n",
    "    return DIC_ADMET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIf='tcm_network/CVA_FJ/sheganmahuangtang/herb_ingredient_pairs_pd.csv'\n",
    "HHf='tcm_network/CVA_FJ/sheganmahuangtang/herb_herb_dis_pd.csv'\n",
    "IIf='tcm_network/CVA_FJ/sheganmahuangtang/ingre_ingre_dis_pd.csv'\n",
    "DHf='tcm_network/CVA_FJ/sheganmahuangtang/herb_disease_pd.csv'\n",
    "DIf='tcm_network/CVA_FJ/sheganmahuangtang/herb_disease_ingre_pd.csv'\n",
    "SGsum_file,DH, DI = file_pre_processing(HIf, HHf, IIf, DHf, DIf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##统计\n",
    "value_counts =SGsum_file['distance'].value_counts()\n",
    "SGstatistical_table = value_counts.reset_index()\n",
    "SGstatistical_table.columns = ['Value', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##裁剪\n",
    "SGsum_file06=SGsum_file[SGsum_file['distance'] < 0.6]\n",
    "G, adj_matrix_np, communities, frames = detect_communities(SGsum_file06)\n",
    "print(list(frames[-1].items())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##聚类结果\n",
    "result,subset_data_I,subset_data_H= communities_result(G,communities,DI,DH,)\n",
    "ADMETf='intergration/data/herb_ingre_info/herb_ingre_tcmsp_pd.xlsx'\n",
    "ADMET = pd.read_excel(ADMETf).drop_duplicates()\n",
    "DI_ADMET= deal_admet_file(subset_data_I,ADMET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
